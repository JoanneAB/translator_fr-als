{"cells":[{"cell_type":"code","execution_count":null,"id":"VcmvaMCFBrqo","metadata":{"id":"VcmvaMCFBrqo"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","my_drive_path = '/content/drive/MyDrive'\n","sys.path.append(my_drive_path)"]},{"cell_type":"code","source":["pip install evaluate bleu sacrebleu"],"metadata":{"id":"yaZ4rQNwXRL6"},"id":"yaZ4rQNwXRL6","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7b211a66","metadata":{"id":"7b211a66"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# BLEU evaluation:\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","from datasets.dataset_dict import DatasetDict\n","from datasets import Dataset\n","import evaluate\n","\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import pickle\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"id":"9af71da1-420d-4336-8f56-bfc8507b1f5f","metadata":{"id":"9af71da1-420d-4336-8f56-bfc8507b1f5f"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline\n","from transformers import NllbTokenizer\n","from transformers import AutoConfig"]},{"cell_type":"code","execution_count":null,"id":"e0ee2fc8","metadata":{"id":"e0ee2fc8"},"outputs":[],"source":["# Load local functions:\n","from database_object import * # read the data and create the dataset\n","from functions import *\n","from functions_model import *"]},{"cell_type":"markdown","source":["# DATA COLLECTION"],"metadata":{"id":"XA4pb3y4neT5"},"id":"XA4pb3y4neT5"},{"cell_type":"code","execution_count":null,"id":"dae54397","metadata":{"id":"dae54397"},"outputs":[],"source":["db = database() # Create a database object\n","\n","# Read the data and fill the database:\n","db.get_data_alsaimmer(display=False)\n","db.get_data_alsatext(display=False)\n","db.get_data_motsAlsacienMulhouse(display=False)\n","db.get_data_alignments(display=False)\n"]},{"cell_type":"code","execution_count":null,"id":"5fc6d11c-feed-414c-a620-e003e49b4138","metadata":{"id":"5fc6d11c-feed-414c-a620-e003e49b4138"},"outputs":[],"source":["# Split the dataset into training/validation/testing subsets :\n","train, validtest = train_test_split(db.db    , test_size=0.6, random_state=0) # 60% training\n","valid, test      = train_test_split(validtest, test_size=0.5, random_state=0) # 20%-20% valid, test\n","\n","# Create the dataset in a 'Dataset' format for tokenization :\n","d = {'train'     : Dataset.from_dict({'translation': train}),\n","     'validation': Dataset.from_dict({'translation': valid}),\n","     'test'      : Dataset.from_dict({'translation': test})\n","     }\n","d = DatasetDict(d)"]},{"cell_type":"markdown","source":["# PRE-PROCESSING"],"metadata":{"id":"hhFRrF5fnlnB"},"id":"hhFRrF5fnlnB"},{"cell_type":"code","execution_count":null,"id":"e1b76d87-3e5b-4df8-8db5-8013df44e2c4","metadata":{"id":"e1b76d87-3e5b-4df8-8db5-8013df44e2c4"},"outputs":[],"source":["# Choice of Tokenizer:\n","checkpoint = \"google-t5/t5-base\" # \"google-t5/t5-small\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","config = AutoConfig.from_pretrained(checkpoint)\n","\n","#checkpoint = \"facebook/nllb-200-distilled-600M\" # too much demanding for colab (RAM excedeed, even with batch_size=1)\n","#checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\" # same\n","#tokenizer = NllbTokenizer.from_pretrained(checkpoint)\n","\n","src_lang = \"fr\"\n","tgt_lang = \"als\" # target language set to german as Alsatian as it is not supported\n","\n","# translate French to Alsatian:\n","tokenizer.src_lang = src_lang\n","tokenizer.tgt_lang = tgt_lang\n","\n","# Print an example of sentence and their tokens:\n","with tokenizer.as_target_tokenizer():\n","  tokens_fr = tokenizer(db.db[0]['fr'])\n","print(\"---- FRENCH ------------------------\")\n","print(db.db[0]['fr'])\n","print(tokens_fr)\n","\n","with tokenizer.as_target_tokenizer():\n","  tokens_als = tokenizer(db.db[0]['als'])\n","print(\"---- ALSACIAN ----------------------\")\n","print(db.db[0]['als'])\n","print(tokens_als)\n"]},{"cell_type":"code","execution_count":null,"id":"e6fde569-7093-4436-b75d-88f943ea5760","metadata":{"id":"e6fde569-7093-4436-b75d-88f943ea5760"},"outputs":[],"source":["tokenized_datasets = d.map(encode, batched=True, fn_kwargs={\"tokenizer\":tokenizer})"]},{"cell_type":"markdown","source":["# MODEL TUNING"],"metadata":{"id":"D9HftmYonu-2"},"id":"D9HftmYonu-2"},{"cell_type":"code","execution_count":null,"id":"90202caa-e862-47fe-9994-3c37b83aa182","metadata":{"id":"90202caa-e862-47fe-9994-3c37b83aa182"},"outputs":[],"source":["# Loading of the pre-trained model for fine-tuning with additional dataset:\n","model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"id":"0b68302a-f5ff-49ee-a110-90041fe6e25a","metadata":{"id":"0b68302a-f5ff-49ee-a110-90041fe6e25a"},"outputs":[],"source":["# Set the output directory for saving the model:\n","model_name = checkpoint.split(\"/\")[-1] # folder name to save output checkpoints\n","\n","output_dir = \"%s/runs/%s_essai6_evaluation\"%(my_drive_path, model_name)"]},{"cell_type":"code","execution_count":null,"id":"023c603a-afe5-4d93-879d-df55128c8c71","metadata":{"id":"023c603a-afe5-4d93-879d-df55128c8c71"},"outputs":[],"source":["# Set the arguments for training :\n","args = Seq2SeqTrainingArguments(\n","    output_dir,\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate = 2e-4,\n","    per_device_train_batch_size = 8, # 64 is too much for colab -> RAM overflow\n","    per_device_eval_batch_size  = 8,\n","    save_steps       = 512, # save every 512 steps.\n","    weight_decay     = 0.01,\n","    save_total_limit = 1,\n","    num_train_epochs = 20,\n","    predict_with_generate = True,\n","    fp16                  = True, # True to speed up training on GPU\n","    metric_for_best_model = 'eval_loss',\n","    greater_is_better     = False,\n","    load_best_model_at_end= True,\n","    seed=1,\n","  )"]},{"cell_type":"code","execution_count":null,"id":"dc28f487-45e0-47f5-9cbb-a0c00adc6242","metadata":{"id":"dc28f487-45e0-47f5-9cbb-a0c00adc6242"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","source":["# Evaluation function\n","# (using references from https://github.com/huggingface/transformers/issues/24433 and https://github.com/huggingface/transformers/issues/22634\n","\n","metric = evaluate.load(\"sacrebleu\")\n","\n","def compute_metrics(eval_pred):\n","    preds, labels = eval_pred\n","    # HF may return a tuple; take first element\n","    if isinstance(preds, (tuple, list)):\n","      preds = preds[0]\n","\n","    # If logits slipped in (B, T, V), convert to token ids safely\n","    if preds.ndim == 3:\n","      preds = np.argmax(preds, axis=-1)\n","\n","    # Map ignore index to a real token id for decoding\n","    pad_id = tokenizer.pad_token_id\n","    preds  = np.where(preds != -100, preds, pad_id)\n","    labels = np.where(labels != -100, labels, pad_id)\n","\n","    pred_seq  = tokenizer.batch_decode(preds,  skip_special_tokens=True)\n","    label_seq = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(pred_seq, label_seq)\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    result = {\"bleu\": result[\"score\"]}\n","    return result"],"metadata":{"id":"3HZvhfussOBU"},"id":"3HZvhfussOBU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MODEL TRAINING"],"metadata":{"id":"-fzlXZmTZrnY"},"id":"-fzlXZmTZrnY"},{"cell_type":"code","execution_count":null,"id":"e27c3f32-2fd7-4be4-bc9b-1c8971c02a5b","metadata":{"id":"e27c3f32-2fd7-4be4-bc9b-1c8971c02a5b"},"outputs":[],"source":["# Set up the trainer:\n","trainer = Seq2SeqTrainer(model, args,\n","    train_dataset    = tokenized_datasets[\"train\"],\n","    eval_dataset     = tokenized_datasets[\"validation\"],\n","    data_collator    = data_collator,\n","    processing_class = tokenizer,\n","    compute_metrics  = compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"id":"55f10116-d64b-4336-9efa-6d7e2fcd8d29","metadata":{"id":"55f10116-d64b-4336-9efa-6d7e2fcd8d29"},"outputs":[],"source":["# Do the training:\n","try:\n","  trainer.train(resume_from_checkpoint=True)\n","except:\n","  trainer.train(resume_from_checkpoint=False)"]},{"cell_type":"markdown","source":["# MODEL EVALUATION"],"metadata":{"id":"YiMSP_VYZwFT"},"id":"YiMSP_VYZwFT"},{"cell_type":"code","execution_count":null,"id":"YsE1dV1L2I8d","metadata":{"id":"YsE1dV1L2I8d"},"outputs":[],"source":["# After training, evaluate the model on test set :\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"id":"OSyDLHnNxvdW","metadata":{"id":"OSyDLHnNxvdW"},"outputs":[],"source":["# Save the model for future use\n","trainer.save_model(output_dir)\n","config.save_pretrained(output_dir)\n","\n","#tokenizer.save_pretrained(output_dir)\n","\n","# save dataset into pickle:\n","with open('%s/tokenized_datasets.pckl'%output_dir, 'wb') as fic:\n","  pickle.dump(tokenized_datasets, fic)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"daRJYPP2ZxgD"},"id":"daRJYPP2ZxgD"},{"cell_type":"code","execution_count":null,"id":"7zKxiMY0e3wx","metadata":{"id":"7zKxiMY0e3wx"},"outputs":[],"source":["# --------------------------------------------------------------------------------------------------\n","def do_translation(text, model_name, return_text=True, return_token=False):\n","  \"\"\"\n","  Do the translation using the trained model.\n","  return either the translation or the tokens\n","  \"\"\"\n","  # make the text a correct format for translation :\n","  translator = pipeline(\"translation_XX_to_YY\", model=model_name)\n","  translator(text)\n","\n","  # Tokenize the text: text -> tokens :\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n","\n","  model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","  # Do the translation using the tokenized text:\n","  outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n","\n","  # Token -> text:\n","  translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","  if return_text:\n","    return translation\n","  if return_tokens:\n","    return output[0]"]},{"cell_type":"markdown","source":["# TRY SOME TRANSLATION"],"metadata":{"id":"dJ664kMoZ1fa"},"id":"dJ664kMoZ1fa"},{"cell_type":"code","execution_count":null,"id":"DlZtCUbZyiIR","metadata":{"id":"DlZtCUbZyiIR"},"outputs":[],"source":["text = \"Le chÃ¢teau est sur la montagne.\"\n","\n","translation = do_translation(text, output_dir)\n","print(\"%s -> %s\"%(text, translation))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":5}